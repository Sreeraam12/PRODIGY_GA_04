# PRODIGY_GA_04
Image-to-Image Translation with cGAN

# Task-4: Image-to-Image Translation with cGAN (Pix2Pix)

In this task, I implemented an **image-to-image translation model** using a **Conditional Generative Adversarial Network (cGAN)** based on the **Pix2Pix** architecture. The goal was to transform one type of image into another, such as turning edge maps into realistic images.

## ðŸ” Objectives
- Understand the architecture of conditional GANs (cGANs)
- Implement the Pix2Pix model for image-to-image translation
- Train and test the model on paired image datasets (e.g., edges2shoes)

## ðŸ› ï¸ Tools & Libraries
- Python
- PyTorch or TensorFlow
- OpenCV / PIL
- Google Colab

## ðŸ“ Contents
- `pix2pix_cgan.ipynb` â€“ Colab notebook for training and inference
- `dataset/` â€“ Sample dataset (e.g., edges and real images)
- `results/` â€“ Output images generated by the model

## ðŸš€ How to Run
1. Open `pix2pix_cgan.ipynb` in Google Colab
2. Upload or link your dataset (paired images)
3. Run training and generate translated images
4. Output will be stored in the `results/` folder

## ðŸ–¼ï¸ Sample Result
> **Input:** Sketch / Edge  
> **Output:** Realistic version generated using cGAN

## ðŸ“Œ Outcome
Successfully translated input images into realistic outputs using Pix2Pix, deepening my understanding of **GAN training dynamics**, **loss functions**, and **image generation techniques**.

---

